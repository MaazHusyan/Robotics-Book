import asyncio
from typing import List, Optional
import hashlib
import time

from openai import OpenAI
from ..utils.config import get_config
from ..utils.errors import AIServiceError


class GeminiEmbeddingService:
    """Service for generating embeddings using Gemini via OpenAI-compatible endpoint."""

    def __init__(self):
        self.settings = get_config()
        self.client = OpenAI(
            api_key=self.settings.GEMINI_API_KEY, base_url=self.settings.OPENAI_BASE_URL
        )
        self._embedding_cache = {}

    async def generate_embedding(self, text: str) -> List[float]:
        """
        Generate embedding for a single text.

        Args:
            text: Text to embed

        Returns:
            List of float values representing the embedding

        Raises:
            EmbeddingError: If embedding generation fails
        """
        if not text or not text.strip():
            raise AIServiceError("Cannot embed empty text")

        # Check cache first
        text_hash = hashlib.md5(text.encode()).hexdigest()
        if text_hash in self._embedding_cache:
            return self._embedding_cache[text_hash]

        try:
            response = self.client.embeddings.create(
                model="text-embedding-3-small", input=text.strip()
            )

            embedding = response.data[0].embedding

            # Cache the result
            self._embedding_cache[text_hash] = embedding

            return embedding

        except Exception as e:
            raise AIServiceError(f"Failed to generate embedding: {str(e)}")

    async def generate_embeddings_batch(
        self, texts: List[str], batch_size: int = 10
    ) -> List[List[float]]:
        """
        Generate embeddings for multiple texts in batches.

        Args:
            texts: List of texts to embed
            batch_size: Number of texts to process in each batch

        Returns:
            List of embeddings corresponding to input texts
        """
        if not texts:
            return []

        embeddings = []

        # Process in batches to avoid rate limits
        for i in range(0, len(texts), batch_size):
            batch = texts[i : i + batch_size]

            try:
                # Generate embeddings for batch
                response = self.client.embeddings.create(
                    model="text-embedding-3-small", input=batch
                )

                batch_embeddings = [data.embedding for data in response.data]
                embeddings.extend(batch_embeddings)

                # Small delay to avoid rate limits
                if i + batch_size < len(texts):
                    await asyncio.sleep(0.5)

            except Exception as e:
                # Fallback to individual generation for failed batch
                print(
                    f"Batch embedding failed, falling back to individual generation: {e}"
                )
                for text in batch:
                    try:
                        embedding = await self.generate_embedding(text)
                        embeddings.append(embedding)
                    except AIServiceError:
                        # Add empty embedding as placeholder
                        embeddings.append([])

        return embeddings

    def get_embedding_dimension(self) -> int:
        """Get the dimension of embeddings generated by this service."""
        return 768  # Gemini text-embedding-3-small generates 768-dimensional embeddings

    def get_cache_stats(self) -> dict:
        """Get statistics about the embedding cache."""
        return {
            "cache_size": len(self._embedding_cache),
            "estimated_memory_mb": len(self._embedding_cache)
            * 768
            * 4
            / (1024 * 1024),  # 4 bytes per float
        }

    def clear_cache(self):
        """Clear the embedding cache."""
        self._embedding_cache.clear()

    async def health_check(self) -> dict:
        """
        Perform a health check on the embedding service.

        Returns:
            Dictionary with health status and metrics
        """
        start_time = time.time()

        try:
            # Test embedding generation
            test_embedding = await self.generate_embedding("test")

            response_time = time.time() - start_time

            return {
                "status": "healthy",
                "response_time_ms": int(response_time * 1000),
                "embedding_dimension": len(test_embedding),
                "cache_stats": self.get_cache_stats(),
            }

        except Exception as e:
            return {
                "status": "unhealthy",
                "error": str(e),
                "response_time_ms": int((time.time() - start_time) * 1000),
            }


# Singleton instance
embedding_service = GeminiEmbeddingService()
