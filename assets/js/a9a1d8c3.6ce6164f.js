"use strict";(globalThis.webpackChunkphysical_and_humanoid_robotics_book=globalThis.webpackChunkphysical_and_humanoid_robotics_book||[]).push([[425],{1343:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>r,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"humanoid-design/balance-gait","title":"Balance, Gait Generation, and Whole-Body Control","description":"Understanding balance mechanisms, gait generation algorithms, and whole-body control strategies for humanoid robots","source":"@site/docs/03-humanoid-design/04-balance-gait.mdx","sourceDirName":"03-humanoid-design","slug":"/humanoid-design/balance-gait","permalink":"/Robotics-Book/docs/humanoid-design/balance-gait","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/03-humanoid-design/04-balance-gait.mdx","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"title":"Balance, Gait Generation, and Whole-Body Control","sidebar_label":"Balance & Gait Control","description":"Understanding balance mechanisms, gait generation algorithms, and whole-body control strategies for humanoid robots"},"sidebar":"bookSidebar","previous":{"title":"Degrees of Freedom and Kinematic Chains","permalink":"/Robotics-Book/docs/humanoid-design/degrees-freedom"},"next":{"title":"Hands & Grippers","permalink":"/Robotics-Book/docs/humanoid-design/hands-grippers"}}');var i=o(4848),s=o(8453);const r={title:"Balance, Gait Generation, and Whole-Body Control",sidebar_label:"Balance & Gait Control",description:"Understanding balance mechanisms, gait generation algorithms, and whole-body control strategies for humanoid robots"},a="Balance, Gait Generation, and Whole-Body Control",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction",id:"introduction",level:2},{value:"Balance Control Fundamentals",id:"balance-control-fundamentals",level:2},{value:"Center of Mass and Center of Pressure",id:"center-of-mass-and-center-of-pressure",level:3},{value:"Zero Moment Point (ZMP) Theory",id:"zero-moment-point-zmp-theory",level:3},{value:"Gait Generation Strategies",id:"gait-generation-strategies",level:2},{value:"Pattern-Based Gait Generation",id:"pattern-based-gait-generation",level:3},{value:"Preview Control Methods",id:"preview-control-methods",level:3},{value:"Central Pattern Generators (CPGs)",id:"central-pattern-generators-cpgs",level:3},{value:"Whole-Body Control",id:"whole-body-control",level:2},{value:"Hierarchical Control Architecture",id:"hierarchical-control-architecture",level:3},{value:"Operational Space Control",id:"operational-space-control",level:3},{value:"Momentum-Based Control",id:"momentum-based-control",level:3},{value:"Sensor Integration for Balance",id:"sensor-integration-for-balance",level:2},{value:"Inertial Measurement Units (IMUs)",id:"inertial-measurement-units-imus",level:3},{value:"Force/Torque Sensors",id:"forcetorque-sensors",level:3},{value:"Practical Implementation Considerations",id:"practical-implementation-considerations",level:2},{value:"Real-Time Constraints",id:"real-time-constraints",level:3},{value:"Safety Considerations",id:"safety-considerations",level:3},{value:"Energy Efficiency",id:"energy-efficiency",level:3},{value:"Advanced Topics",id:"advanced-topics",level:2},{value:"Learning-Based Balance Control",id:"learning-based-balance-control",level:3},{value:"Multi-Contact Locomotion",id:"multi-contact-locomotion",level:3},{value:"Summary",id:"summary",level:2},{value:"References",id:"references",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",input:"input",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"balance-gait-generation-and-whole-body-control",children:"Balance, Gait Generation, and Whole-Body Control"})}),"\n",(0,i.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,i.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Understand fundamental principles of balance control for humanoid robots"]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Master Zero Moment Point (ZMP) theory and its applications"]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Analyze different gait generation strategies and algorithms"]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Implement whole-body control for coordinated movement"]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Design balance and gait systems for practical applications"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,i.jsx)(n.p,{children:"Balance and gait generation represent two of the most challenging aspects of humanoid robotics. Unlike wheeled robots that maintain stability through a wide base of support, humanoid robots must constantly manage their center of mass to prevent falling while walking, standing, or performing complex movements. This module explores the fundamental principles of balance control, gait generation algorithms, and whole-body coordination strategies that enable humanoid robots to move with human-like stability and grace."}),"\n",(0,i.jsx)(n.h2,{id:"balance-control-fundamentals",children:"Balance Control Fundamentals"}),"\n",(0,i.jsx)(n.h3,{id:"center-of-mass-and-center-of-pressure",children:"Center of Mass and Center of Pressure"}),"\n",(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.strong,{children:"Center of Mass (CoM)"})," represents the average position of all mass in the robot's body, while the ",(0,i.jsx)(n.strong,{children:"Center of Pressure (CoP)"})," is the point where the total sum of a pressure field acts on a body. For stable standing, the CoP must remain within the support polygon formed by the robot's feet."]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Key Principles:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"The CoM projection must stay within the base of support for static stability"}),"\n",(0,i.jsx)(n.li,{children:"During dynamic movement, the CoP can move outside the support polygon temporarily"}),"\n",(0,i.jsx)(n.li,{children:"The relationship between CoM and CoP determines the robot's stability state"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"zero-moment-point-zmp-theory",children:"Zero Moment Point (ZMP) Theory"}),"\n",(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.strong,{children:"Zero Moment Point (ZMP)"})," is a fundamental concept in bipedal locomotion. The ZMP is the point on the ground where the total moment of all forces acting on the robot equals zero. For stable dynamic walking, the ZMP must remain within the support polygon."]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"ZMP Calculation:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"ZMP_x = (\u03a3(F_i * z_i) - m * g * x_com) / (\u03a3(F_i))\nZMP_y = (\u03a3(F_i * z_i) - m * g * y_com) / (\u03a3(F_i))\n"})}),"\n",(0,i.jsx)(n.p,{children:"Where:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"F_i"})," represents ground reaction forces"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"z_i"})," is the height of force application"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"m"})," is the robot's total mass"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"g"})," is gravitational acceleration"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"x_com"}),", ",(0,i.jsx)(n.code,{children:"y_com"})," are center of mass coordinates"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"gait-generation-strategies",children:"Gait Generation Strategies"}),"\n",(0,i.jsx)(n.h3,{id:"pattern-based-gait-generation",children:"Pattern-Based Gait Generation"}),"\n",(0,i.jsx)(n.p,{children:"Pattern-based approaches use predefined motion patterns that are modified based on sensor feedback. These methods are computationally efficient and reliable for known terrains."}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Common Gait Patterns:"})}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Static Walking"}),": The robot maintains static equilibrium throughout the gait cycle"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Dynamic Walking"}),": The robot uses dynamic stability with controlled falling phases"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Hybrid Walking"}),": Combines static and dynamic phases for optimal efficiency"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"preview-control-methods",children:"Preview Control Methods"}),"\n",(0,i.jsx)(n.p,{children:"Preview control uses future reference trajectories to optimize current control inputs. This approach is particularly effective for humanoid robots because it can anticipate upcoming balance challenges."}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Linear Quadratic Regulator (LQR) with Preview:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import numpy as np\nfrom scipy import linalg\n\nclass PreviewController:\n    def __init__(self, horizon=20):\n        self.horizon = horizon\n        self.A, self.B = self._build_model_matrices()\n        self.Q, self.R = self._build_cost_matrices()\n        self.K = self._compute_lqr_gain()\n        \n    def _build_model_matrices(self):\n        # Simplified inverted pendulum model\n        dt = 0.01  # 10ms control period\n        g = 9.81\n        z_c = 0.8  # COM height\n        \n        A = np.array([[1, dt, 0],\n                      [0, 1, dt],\n                      [0, 0, 1]])\n        B = np.array([[0], [dt**2/z_c], [dt]])\n        return A, B\n    \n    def compute_control(self, state, preview_trajectory):\n        control = -self.K @ state\n        for i in range(min(self.horizon, len(preview_trajectory))):\n            control += self._preview_gain(i) @ preview_trajectory[i]\n        return control\n"})}),"\n",(0,i.jsx)(n.h3,{id:"central-pattern-generators-cpgs",children:"Central Pattern Generators (CPGs)"}),"\n",(0,i.jsx)(n.p,{children:"Central Pattern Generators are neural networks that produce rhythmic patterns without requiring sensory feedback. CPGs can generate adaptive gaits that respond to environmental changes."}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"CPG Implementation Example:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"class CPGNetwork:\n    def __init__(self, num_oscillators=4):\n        self.num_oscillators = num_oscillators\n        self.frequencies = np.ones(num_oscillators) * 2.0  # 2 Hz base frequency\n        self.amplitudes = np.ones(num_oscillators) * 0.1\n        self.phase_offsets = np.array([0, np.pi, np.pi/2, 3*np.pi/2])\n        self.coupling_weights = self._initialize_coupling()\n        \n    def step(self, dt):\n        # Update oscillator phases\n        self.phases += 2 * np.pi * self.frequencies * dt\n        self.phases = self.phases % (2 * np.pi)\n        \n        # Apply coupling between oscillators\n        for i in range(self.num_oscillators):\n            for j in range(self.num_oscillators):\n                if i != j:\n                    phase_diff = self.phases[j] - self.phases[i]\n                    self.phases[i] += self.coupling_weights[i,j] * np.sin(phase_diff) * dt\n        \n        # Generate output signals\n        outputs = self.amplitudes * np.sin(self.phases + self.phase_offsets)\n        return outputs\n"})}),"\n",(0,i.jsx)(n.h2,{id:"whole-body-control",children:"Whole-Body Control"}),"\n",(0,i.jsx)(n.h3,{id:"hierarchical-control-architecture",children:"Hierarchical Control Architecture"}),"\n",(0,i.jsx)(n.p,{children:"Whole-body control for humanoid robots typically follows a hierarchical structure:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"High-Level Planning"}),": Task planning and trajectory generation"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Mid-Level Coordination"}),": Balance control and gait generation"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Low-Level Control"}),": Joint torque/position control"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"operational-space-control",children:"Operational Space Control"}),"\n",(0,i.jsx)(n.p,{children:"Operational Space Control (OSC) allows the robot to control specific tasks in Cartesian space while managing redundancy through null space projection."}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"OSC Implementation:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"class OperationalSpaceController:\n    def __init__(self, robot_model):\n        self.robot = robot_model\n        self.lambda_reg = 0.01  # Regularization parameter\n        \n    def compute_command(self, task_space_accel, joint_positions, joint_velocities):\n        # Get current robot state\n        J = self.robot.jacobian(joint_positions)  # Task Jacobian\n        M = self.robot.mass_matrix(joint_positions)  # Joint space inertia\n        \n        # Compute operational space dynamics\n        Lambda = np.linalg.inv(J @ np.linalg.inv(M) @ J.T + self.lambda_reg * np.eye(J.shape[0]))\n        \n        # Compute task space control\n        task_force = Lambda @ task_space_accel\n        \n        # Project to joint space\n        joint_torques = J.T @ task_force\n        \n        # Add null space control for posture maintenance\n        J_pinv = np.linalg.pinv(J)\n        null_space = np.eye(J.shape[1]) - J_pinv @ J\n        posture_control = -null_space @ (10 * joint_positions + 5 * joint_velocities)\n        \n        return joint_torques + posture_control\n"})}),"\n",(0,i.jsx)(n.h3,{id:"momentum-based-control",children:"Momentum-Based Control"}),"\n",(0,i.jsx)(n.p,{children:"Momentum-based control explicitly manages the robot's linear and angular momentum, which is crucial for dynamic balance during complex movements."}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Momentum Control Equations:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"L_dot = F_ext + m * g\nH_dot = \u03c4_ext + r \xd7 F_ext\n"})}),"\n",(0,i.jsx)(n.p,{children:"Where:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"L"})," is linear momentum"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"H"})," is angular momentum"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"F_ext"})," represents external forces"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"\u03c4_ext"})," represents external torques"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"r"})," is the position vector from CoM to force application point"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"sensor-integration-for-balance",children:"Sensor Integration for Balance"}),"\n",(0,i.jsx)(n.h3,{id:"inertial-measurement-units-imus",children:"Inertial Measurement Units (IMUs)"}),"\n",(0,i.jsx)(n.p,{children:"IMUs provide critical information about the robot's orientation and acceleration. Proper sensor fusion is essential for accurate state estimation."}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"IMU Data Processing:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"class IMUProcessor:\n    def __init__(self):\n        self.gyro_bias = np.zeros(3)\n        self.accel_bias = np.zeros(3)\n        self.orientation = np.array([0, 0, 0, 1])  # Quaternion [x, y, z, w]\n        \n    def update_orientation(self, gyro, accel, dt):\n        # Remove bias\n        gyro_corrected = gyro - self.gyro_bias\n        accel_corrected = accel - self.accel_bias\n        \n        # Complementary filter for orientation estimation\n        alpha = 0.98  # Gyro weight\n        orientation_gyro = self._integrate_gyro(gyro_corrected, dt)\n        orientation_accel = self._estimate_from_accel(accel_corrected)\n        \n        self.orientation = alpha * orientation_gyro + (1 - alpha) * orientation_accel\n        self.orientation = self._normalize_quaternion(self.orientation)\n        \n    def get_angular_velocity(self):\n        return self._quaternion_to_angular_velocity(self.orientation)\n"})}),"\n",(0,i.jsx)(n.h3,{id:"forcetorque-sensors",children:"Force/Torque Sensors"}),"\n",(0,i.jsx)(n.p,{children:"Force/torque sensors in the robot's feet provide direct measurement of ground reaction forces, enabling precise ZMP calculation."}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"ZMP from Force Sensors:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'def compute_zmp_from_sensors(force_torque_data):\n    """\n    Compute ZMP from foot force/torque sensor data\n    """\n    # Extract force and moment components\n    fx, fy, fz = force_torque_data[\'force\']\n    mx, my, mz = force_torque_data[\'moment\']\n    \n    # Compute ZMP coordinates (assuming foot frame)\n    if abs(fz) > 0.01:  # Avoid division by zero\n        zmp_x = -my / fz\n        zmp_y = mx / fz\n    else:\n        zmp_x, zmp_y = 0, 0\n    \n    return np.array([zmp_x, zmp_y])\n'})}),"\n",(0,i.jsx)(n.h2,{id:"practical-implementation-considerations",children:"Practical Implementation Considerations"}),"\n",(0,i.jsx)(n.h3,{id:"real-time-constraints",children:"Real-Time Constraints"}),"\n",(0,i.jsx)(n.p,{children:"Balance control must operate at high frequencies (typically 500-1000 Hz) to respond quickly to disturbances. This requires efficient algorithms and optimized code."}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Optimization Strategies:"})}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Precomputed Matrices"}),": Cache frequently used matrix computations"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Reduced-Order Models"}),": Use simplified models for control calculations"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Parallel Processing"}),": Distribute computations across multiple CPU cores"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"safety-considerations",children:"Safety Considerations"}),"\n",(0,i.jsx)(n.p,{children:"Humanoid robots pose significant safety risks during balance loss events. Robust safety systems are essential."}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Safety Mechanisms:"})}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Emergency Stop"}),": Immediate joint locking when dangerous conditions detected"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Safe Falling"}),": Controlled falling strategies to minimize damage"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Collision Detection"}),": Real-time monitoring for unexpected contacts"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"energy-efficiency",children:"Energy Efficiency"}),"\n",(0,i.jsx)(n.p,{children:"Energy consumption is a critical constraint for mobile humanoid robots."}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Energy Optimization Techniques:"})}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Passive Dynamics"}),": Utilize natural dynamics to reduce actuator requirements"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Regenerative Braking"}),": Capture energy during deceleration phases"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Optimal Gait Selection"}),": Choose gaits that minimize energy consumption for given tasks"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"advanced-topics",children:"Advanced Topics"}),"\n",(0,i.jsx)(n.h3,{id:"learning-based-balance-control",children:"Learning-Based Balance Control"}),"\n",(0,i.jsx)(n.p,{children:"Machine learning approaches can adapt to changing conditions and improve performance over time."}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Reinforcement Learning for Balance:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import gym\nimport numpy as np\nfrom stable_baselines3 import PPO\n\nclass HumanoidBalanceEnv(gym.Env):\n    def __init__(self):\n        super().__init__()\n        self.action_space = gym.spaces.Box(low=-1, high=1, shape=(12,))  # 12 joint torques\n        self.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=(24,))\n        \n    def step(self, action):\n        # Apply joint torques\n        self.robot.set_joint_torques(action)\n        \n        # Simulate physics\n        self.robot.simulate_step(self.dt)\n        \n        # Compute reward based on balance and task completion\n        reward = self._compute_balance_reward()\n        \n        # Check termination conditions\n        done = self._check_fall() or self._task_complete()\n        \n        return self._get_observation(), reward, done, {}\n    \n    def _compute_balance_reward(self):\n        # Reward for maintaining upright posture\n        orientation_error = np.linalg.norm(self.robot.get_orientation_error())\n        zmp_margin = self.robot.get_zmp_stability_margin()\n        \n        balance_reward = np.exp(-orientation_error) * zmp_margin\n        return balance_reward\n"})}),"\n",(0,i.jsx)(n.h3,{id:"multi-contact-locomotion",children:"Multi-Contact Locomotion"}),"\n",(0,i.jsx)(n.p,{children:"Advanced humanoid robots can use walls, handrails, or other environmental contacts for enhanced stability."}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Contact Planning:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"class ContactPlanner:\n    def __init__(self, robot_model):\n        self.robot = robot_model\n        self.contact_sequence = []\n        \n    def plan_contacts(self, goal_pose, environment):\n        # Generate potential contact locations\n        potential_contacts = self._find_contact_surfaces(environment)\n        \n        # Plan contact sequence using graph search\n        contact_graph = self._build_contact_graph(potential_contacts)\n        optimal_sequence = self._find_optimal_sequence(contact_graph, goal_pose)\n        \n        return optimal_sequence\n    \n    def _find_contact_surfaces(self, environment):\n        # Identify surfaces suitable for contact\n        contacts = []\n        for surface in environment.surfaces:\n            if self._is_valid_contact(surface):\n                contacts.append(surface)\n        return contacts\n"})}),"\n",(0,i.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,i.jsx)(n.p,{children:"Balance, gait generation, and whole-body control are fundamental capabilities that enable humanoid robots to operate effectively in human environments. Key takeaways include:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"ZMP Theory"}),": The Zero Moment Point provides a mathematical framework for understanding dynamic stability"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Hierarchical Control"}),": Multi-layered control architectures enable complex behaviors while maintaining stability"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Sensor Integration"}),": Accurate state estimation from multiple sensors is essential for robust balance"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Adaptive Algorithms"}),": Modern approaches use learning and adaptation to handle uncertain environments"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Safety and Efficiency"}),": Practical implementations must consider safety constraints and energy optimization"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"The field continues to evolve with advances in machine learning, sensor technology, and computational power, bringing us closer to humanoid robots that can move with the grace and reliability of their human counterparts."}),"\n",(0,i.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:'Kajita, S., et al. "Introduction to Humanoid Robotics." Springer, 2014.'}),"\n",(0,i.jsx)(n.li,{children:'Vukobratovi\u0107, M., & Borovac, B. "Zero-moment point\u2014Thirty five years of its life." International Journal of Humanoid Robotics, 2004.'}),"\n",(0,i.jsx)(n.li,{children:'Englsberger, J., et al. "Overview of the torque-controlled humanoid robot TORO." IEEE-RAS International Conference on Humanoid Robots, 2014.'}),"\n",(0,i.jsx)(n.li,{children:'Tedrake, R., et al. "LQR-Trees: Feedback motion planning via sums-of-squares verification." International Journal of Robotics Research, 2010.'}),"\n",(0,i.jsx)(n.li,{children:'Kuindersma, S., et al. "Optimization-based locomotion planning, estimation, and control design for the Atlas humanoid robot." Autonomous Robots, 2016.'}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,n,o)=>{o.d(n,{R:()=>r,x:()=>a});var t=o(6540);const i={},s=t.createContext(i);function r(e){const n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);