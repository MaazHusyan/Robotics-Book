"use strict";(globalThis.webpackChunkphysical_and_humanoid_robotics_book=globalThis.webpackChunkphysical_and_humanoid_robotics_book||[]).push([[97],{2460:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>p,frontMatter:()=>t,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"physical-fundamentals/sensors","title":"Sensors and Perception","description":"Comprehensive guide to robotic sensors and perception systems for humanoid robots, covering proprioception, exteroception, and sensor fusion","source":"@site/docs/02-physical-fundamentals/03-sensors.mdx","sourceDirName":"02-physical-fundamentals","slug":"/physical-fundamentals/sensors","permalink":"/Robotics-Book/docs/physical-fundamentals/sensors","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/02-physical-fundamentals/03-sensors.mdx","tags":[{"inline":true,"label":"sensors","permalink":"/Robotics-Book/docs/tags/sensors"},{"inline":true,"label":"perception","permalink":"/Robotics-Book/docs/tags/perception"},{"inline":true,"label":"robotics","permalink":"/Robotics-Book/docs/tags/robotics"},{"inline":true,"label":"physical-fundamentals","permalink":"/Robotics-Book/docs/tags/physical-fundamentals"}],"version":"current","sidebarPosition":3,"frontMatter":{"title":"Sensors and Perception","sidebar_label":"Sensors and Perception","description":"Comprehensive guide to robotic sensors and perception systems for humanoid robots, covering proprioception, exteroception, and sensor fusion","hide_table_of_contents":false,"authors":["Robotics Book Team"],"tags":["sensors","perception","robotics","physical-fundamentals"],"reading_time":25,"difficulty":"intermediate","prerequisites":["01-kinematics-dynamics","02-actuators-motors"],"learning_objectives":["Understand different types of sensors used in humanoid robotics","Compare proprioceptive and exteroceptive sensing systems","Analyze sensor characteristics and selection criteria","Implement sensor fusion for robust perception","Design sensor placement for optimal humanoid robot perception"]},"sidebar":"bookSidebar","previous":{"title":"Actuators and Motors","permalink":"/Robotics-Book/docs/physical-fundamentals/actuators-motors"},"next":{"title":"Power Systems and Energy Management","permalink":"/Robotics-Book/docs/physical-fundamentals/power-systems"}}');var r=s(4848),o=s(8453);const t={title:"Sensors and Perception",sidebar_label:"Sensors and Perception",description:"Comprehensive guide to robotic sensors and perception systems for humanoid robots, covering proprioception, exteroception, and sensor fusion",hide_table_of_contents:!1,authors:["Robotics Book Team"],tags:["sensors","perception","robotics","physical-fundamentals"],reading_time:25,difficulty:"intermediate",prerequisites:["01-kinematics-dynamics","02-actuators-motors"],learning_objectives:["Understand different types of sensors used in humanoid robotics","Compare proprioceptive and exteroceptive sensing systems","Analyze sensor characteristics and selection criteria","Implement sensor fusion for robust perception","Design sensor placement for optimal humanoid robot perception"]},a="Sensors and Perception",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction",id:"introduction",level:2},{value:"Proprioceptive Sensors",id:"proprioceptive-sensors",level:2},{value:"Joint Position Sensors",id:"joint-position-sensors",level:3},{value:"Force and Torque Sensors",id:"force-and-torque-sensors",level:3},{value:"Inertial Measurement Units (IMUs)",id:"inertial-measurement-units-imus",level:3},{value:"Exteroceptive Sensors",id:"exteroceptive-sensors",level:2},{value:"Vision Systems",id:"vision-systems",level:3},{value:"Range Finding Sensors",id:"range-finding-sensors",level:3},{value:"Tactile Sensors",id:"tactile-sensors",level:3},{value:"Sensor Characteristics and Selection",id:"sensor-characteristics-and-selection",level:2},{value:"Performance Metrics",id:"performance-metrics",level:3},{value:"Environmental Considerations",id:"environmental-considerations",level:3},{value:"Cost and Complexity",id:"cost-and-complexity",level:3},{value:"Sensor Fusion",id:"sensor-fusion",level:2},{value:"Kalman Filtering",id:"kalman-filtering",level:3},{value:"Complementary Filtering",id:"complementary-filtering",level:3},{value:"Particle Filtering",id:"particle-filtering",level:3},{value:"Sensor Placement Strategies",id:"sensor-placement-strategies",level:2},{value:"Humanoid Robot Sensor Layout",id:"humanoid-robot-sensor-layout",level:3},{value:"Redundancy and Robustness",id:"redundancy-and-robustness",level:3},{value:"Code Examples",id:"code-examples",level:2},{value:"Sensor Data Processing Pipeline",id:"sensor-data-processing-pipeline",level:3},{value:"Vision Processing for Object Detection",id:"vision-processing-for-object-detection",level:3},{value:"Practical Examples",id:"practical-examples",level:2},{value:"Humanoid Robot Balance Control",id:"humanoid-robot-balance-control",level:3},{value:"Exercises",id:"exercises",level:2},{value:"Exercise 1: Sensor Selection Analysis",id:"exercise-1-sensor-selection-analysis",level:3},{value:"Exercise 2: Sensor Fusion Implementation",id:"exercise-2-sensor-fusion-implementation",level:3},{value:"Exercise 3: Sensor Calibration",id:"exercise-3-sensor-calibration",level:3},{value:"Exercise 4: Fault Detection",id:"exercise-4-fault-detection",level:3},{value:"Summary",id:"summary",level:2},{value:"References",id:"references",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",input:"input",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"sensors-and-perception",children:"Sensors and Perception"})}),"\n",(0,r.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,r.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Understand different types of sensors used in humanoid robotics"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Compare proprioceptive and exteroceptive sensing systems"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Analyze sensor characteristics and selection criteria"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Implement sensor fusion for robust perception"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Design sensor placement for optimal humanoid robot perception"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,r.jsx)(n.p,{children:"Sensors serve as the sensory organs of humanoid robots, enabling them to perceive themselves and their environment. Just as humans rely on vision, touch, balance, and proprioception to navigate and interact with the world, humanoid robots require sophisticated sensor systems to achieve similar capabilities. These sensors provide the raw data that robots use to understand their internal state, detect obstacles, recognize objects, and make intelligent decisions."}),"\n",(0,r.jsx)(n.p,{children:"The complexity of humanoid locomotion and manipulation demands multiple, complementary sensing modalities. A humanoid robot must know the position of its joints, the forces it's applying, its orientation in space, and the location of objects around it. This module explores the diverse sensor technologies that make such perception possible, from simple encoders to complex vision systems."}),"\n",(0,r.jsx)(n.h2,{id:"proprioceptive-sensors",children:"Proprioceptive Sensors"}),"\n",(0,r.jsx)(n.p,{children:"Proprioceptive sensors provide information about the robot's internal state, including joint positions, velocities, forces, and overall orientation. They are essential for closed-loop control and enable robots to perform precise movements despite disturbances and uncertainties."}),"\n",(0,r.jsx)(n.h3,{id:"joint-position-sensors",children:"Joint Position Sensors"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Rotary Encoders"})," measure angular position and velocity of robot joints. They come in two main varieties:"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Incremental Encoders"})," output pulses as the shaft rotates, requiring a reference position to determine absolute angle. They offer high resolution and are cost-effective but lose position information during power loss."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Absolute Encoders"})," provide a unique digital code for each angular position, maintaining position information even after power cycling. They are more expensive but eliminate the need for homing procedures."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Resolution and Accuracy"}),": Encoder resolution determines the smallest detectable angular change. High-precision applications may require resolutions of 20 bits or more, providing approximately one million discrete positions per revolution."]}),"\n",(0,r.jsx)(n.h3,{id:"force-and-torque-sensors",children:"Force and Torque Sensors"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Strain Gauge Sensors"})," measure deformation of materials under load. When force is applied, the strain gauge's electrical resistance changes proportionally, allowing precise force measurement."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Six-Axis Force/Torque Sensors"})," measure forces and torques along all three Cartesian axes (Fx, Fy, Fz, Tx, Ty, Tz). These sensors are crucial for:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Force-controlled manipulation"}),"\n",(0,r.jsx)(n.li,{children:"Collision detection"}),"\n",(0,r.jsx)(n.li,{children:"Walking on uneven terrain"}),"\n",(0,r.jsx)(n.li,{children:"Safe human-robot interaction"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Capacitive Force Sensors"})," use changes in capacitance to measure force. They offer advantages in certain applications due to their compact size and low power consumption."]}),"\n",(0,r.jsx)(n.h3,{id:"inertial-measurement-units-imus",children:"Inertial Measurement Units (IMUs)"}),"\n",(0,r.jsx)(n.p,{children:"IMUs combine multiple sensors to measure orientation and motion:"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Accelerometers"})," measure linear acceleration along three axes. They detect gravity direction, allowing estimation of tilt angle when stationary."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Gyroscopes"})," measure angular velocity around three axes. They provide information about rotational motion and help track orientation changes."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Magnetometers"})," measure magnetic field strength, typically used to determine heading relative to Earth's magnetic field."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Sensor Fusion"}),": Raw IMU data is noisy and drifts over time. Complementary filters or Kalman filters combine accelerometer and gyroscope data to provide accurate orientation estimates."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import numpy as np\n\nclass IMUFilter:\n    """Complementary filter for IMU sensor fusion"""\n    \n    def __init__(self, alpha=0.98):\n        self.alpha = alpha  # Complementary filter coefficient\n        self.angle = np.array([0.0, 0.0, 0.0])  # Roll, pitch, yaw\n        \n    def update(self, accel_data, gyro_data, dt):\n        """Update orientation estimate using complementary filter"""\n        # Integrate gyroscope data\n        gyro_angle = self.angle + gyro_data * dt\n        \n        # Calculate angle from accelerometer (roll and pitch only)\n        accel_x, accel_y, accel_z = accel_data\n        accel_angle = np.array([\n            np.arctan2(accel_y, np.sqrt(accel_x**2 + accel_z**2)),\n            np.arctan2(-accel_x, np.sqrt(accel_y**2 + accel_z**2)),\n            self.angle[2]  # Yaw from magnetometer or integration only\n        ])\n        \n        # Complementary filter: combine gyroscope and accelerometer\n        self.angle = self.alpha * gyro_angle + (1 - self.alpha) * accel_angle\n        \n        return self.angle\n\n# Example usage\nimu_filter = IMUFilter(alpha=0.98)\n\n# Simulate sensor data\ndt = 0.01  # 100Hz sampling\ntime_steps = 1000\n\nfor i in range(time_steps):\n    # Simulated sensor readings (would come from actual IMU)\n    accel_data = np.array([0.1, 0.2, 9.8])  # Slight tilt\n    gyro_data = np.array([0.01, -0.02, 0.0])  # Small rotation\n    \n    orientation = imu_filter.update(accel_data, gyro_data, dt)\n    \n    if i % 100 == 0:\n        print(f"Time {i*dt:.2f}s: Roll={np.degrees(orientation[0]):.1f}\xb0, "\n              f"Pitch={np.degrees(orientation[1]):.1f}\xb0")\n'})}),"\n",(0,r.jsx)(n.h2,{id:"exteroceptive-sensors",children:"Exteroceptive Sensors"}),"\n",(0,r.jsx)(n.p,{children:"Exteroceptive sensors provide information about the external environment, including obstacles, objects, surfaces, and humans. These sensors enable robots to navigate safely and interact intelligently with their surroundings."}),"\n",(0,r.jsx)(n.h3,{id:"vision-systems",children:"Vision Systems"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Monocular Cameras"})," provide 2D image data. While they don't directly provide depth information, they can be used with computer vision algorithms for:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Object recognition and tracking"}),"\n",(0,r.jsx)(n.li,{children:"Feature detection and matching"}),"\n",(0,r.jsx)(n.li,{children:"Visual odometry"}),"\n",(0,r.jsx)(n.li,{children:"Human pose estimation"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Stereo Cameras"})," use two cameras separated by a known baseline to compute depth through triangulation. They provide 3D point clouds and enable:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Depth mapping"}),"\n",(0,r.jsx)(n.li,{children:"Obstacle avoidance"}),"\n",(0,r.jsx)(n.li,{children:"3D reconstruction"}),"\n",(0,r.jsx)(n.li,{children:"Distance measurement"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Depth Cameras"})," use various technologies to directly measure depth:"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Time-of-Flight (ToF) Cameras"})," measure the time it takes for light to travel to objects and back, providing depth maps at high frame rates."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Structured Light Cameras"})," project known patterns and analyze their deformation to compute depth, offering high resolution at shorter ranges."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"RGB-D Cameras"})," combine color images with depth information, providing rich data for perception algorithms."]}),"\n",(0,r.jsx)(n.h3,{id:"range-finding-sensors",children:"Range Finding Sensors"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"LiDAR (Light Detection and Ranging)"})," uses laser pulses to measure distances to objects. LiDAR systems provide:"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"2D LiDAR"})," scans a single plane, creating 2D point clouds useful for:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Floor-level navigation"}),"\n",(0,r.jsx)(n.li,{children:"Obstacle detection"}),"\n",(0,r.jsx)(n.li,{children:"Localization and mapping"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"3D LiDAR"})," scans multiple planes or uses rotating mechanisms to create 3D point clouds, enabling:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Comprehensive environment mapping"}),"\n",(0,r.jsx)(n.li,{children:"Object detection and tracking"}),"\n",(0,r.jsx)(n.li,{children:"Autonomous navigation"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Ultrasonic Sensors"})," emit sound waves and measure echo return time. They offer:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Low-cost distance measurement"}),"\n",(0,r.jsx)(n.li,{children:"Robustness to lighting conditions"}),"\n",(0,r.jsx)(n.li,{children:"Simple operation"}),"\n",(0,r.jsx)(n.li,{children:"Limited range and resolution"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"tactile-sensors",children:"Tactile Sensors"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Pressure Sensors"})," measure contact force distribution. They enable:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Grasp force control"}),"\n",(0,r.jsx)(n.li,{children:"Slip detection"}),"\n",(0,r.jsx)(n.li,{children:"Texture recognition"}),"\n",(0,r.jsx)(n.li,{children:"Safe human interaction"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Force-Sensing Resistors (FSRs)"})," change resistance with applied force, providing simple, low-cost pressure sensing."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Capacitive Tactile Sensors"})," detect changes in capacitance caused by pressure, offering high sensitivity and multi-touch capability."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Optical Tactile Sensors"})," use cameras and deformable surfaces to provide rich tactile information, including:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Contact shape"}),"\n",(0,r.jsx)(n.li,{children:"Force distribution"}),"\n",(0,r.jsx)(n.li,{children:"Slip detection"}),"\n",(0,r.jsx)(n.li,{children:"Texture analysis"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"sensor-characteristics-and-selection",children:"Sensor Characteristics and Selection"}),"\n",(0,r.jsx)(n.h3,{id:"performance-metrics",children:"Performance Metrics"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Resolution"}),": The smallest detectable change in the measured quantity. Higher resolution provides more detailed information but may increase computational requirements."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Accuracy"}),": How close measurements are to true values. Accuracy depends on calibration, environmental conditions, and sensor quality."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Precision"}),": Repeatability of measurements. High precision ensures consistent readings even if they're not perfectly accurate."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Range"}),": Minimum and maximum measurable values. Sensors must be selected to cover expected operating ranges."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Bandwidth"}),": How quickly sensors can respond to changes. High bandwidth is essential for dynamic applications like walking."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Latency"}),": Time delay between physical change and sensor output. Low latency is crucial for real-time control."]}),"\n",(0,r.jsx)(n.h3,{id:"environmental-considerations",children:"Environmental Considerations"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Temperature Range"}),": Sensors must operate reliably across expected temperature variations."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Vibration Resistance"}),": Mobile robots experience significant vibrations that can affect sensor performance."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Electromagnetic Interference"}),": Motors and power electronics can interfere with sensitive sensors."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Lighting Conditions"}),": Vision systems must handle varying lighting conditions, from bright sunlight to dark environments."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Weather Protection"}),": Outdoor robots require weatherproof sensors with appropriate IP ratings."]}),"\n",(0,r.jsx)(n.h3,{id:"cost-and-complexity",children:"Cost and Complexity"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Initial Cost"}),": Purchase price of sensors and required electronics."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Integration Cost"}),": Effort required to integrate sensors into the robot system."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Calibration Requirements"}),": Some sensors require frequent calibration to maintain accuracy."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Computational Requirements"}),": Processing sensor data may require significant computational resources."]}),"\n",(0,r.jsx)(n.h2,{id:"sensor-fusion",children:"Sensor Fusion"}),"\n",(0,r.jsx)(n.p,{children:"Sensor fusion combines data from multiple sensors to provide more accurate and robust information than any single sensor could provide alone. This is essential because each sensor type has strengths and weaknesses."}),"\n",(0,r.jsx)(n.h3,{id:"kalman-filtering",children:"Kalman Filtering"}),"\n",(0,r.jsx)(n.p,{children:"Kalman filters provide optimal estimates of system state by combining predictions with measurements. They are widely used in robotics for:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Position and orientation estimation"}),"\n",(0,r.jsx)(n.li,{children:"Velocity and acceleration estimation"}),"\n",(0,r.jsx)(n.li,{children:"Tracking moving objects"}),"\n",(0,r.jsx)(n.li,{children:"Sensor noise reduction"}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import numpy as np\n\nclass KalmanFilter:\n    """Extended Kalman Filter for sensor fusion"""\n    \n    def __init__(self, state_dim, measurement_dim):\n        # State variables\n        self.state = np.zeros(state_dim)\n        self.covariance = np.eye(state_dim)\n        \n        # Process model\n        self.state_transition = np.eye(state_dim)\n        self.process_noise = np.eye(state_dim) * 0.01\n        \n        # Measurement model\n        self.measurement_matrix = np.zeros((measurement_dim, state_dim))\n        self.measurement_noise = np.eye(measurement_dim) * 0.1\n        \n    def predict(self, control_input=None):\n        """Predict next state using process model"""\n        if control_input is not None:\n            self.state = self.state_transition @ self.state + control_input\n        else:\n            self.state = self.state_transition @ self.state\n            \n        self.covariance = (self.state_transition @ self.covariance @ \n                          self.state_transition.T + self.process_noise)\n        \n        return self.state\n    \n    def update(self, measurement):\n        """Update state estimate with new measurement"""\n        # Calculate Kalman gain\n        innovation_covariance = (self.measurement_matrix @ self.covariance @ \n                                self.measurement_matrix.T + self.measurement_noise)\n        kalman_gain = (self.covariance @ self.measurement_matrix.T @ \n                      np.linalg.inv(innovation_covariance))\n        \n        # Update state and covariance\n        innovation = measurement - self.measurement_matrix @ self.state\n        self.state = self.state + kalman_gain @ innovation\n        self.covariance = ((np.eye(len(self.state)) - kalman_gain @ \n                           self.measurement_matrix) @ self.covariance)\n        \n        return self.state\n\n# Example: Fusing encoder and IMU data for position estimation\nclass RobotPositionEstimator:\n    def __init__(self):\n        # State: [x, y, theta, vx, vy, vtheta]\n        self.kf = KalmanFilter(state_dim=6, measurement_dim=4)\n        \n        # Measurement matrix: [encoder_x, encoder_y, imu_vx, imu_vy]\n        self.kf.measurement_matrix = np.array([\n            [1, 0, 0, 0, 0, 0],  # encoder x position\n            [0, 1, 0, 0, 0, 0],  # encoder y position\n            [0, 0, 0, 1, 0, 0],  # IMU x velocity\n            [0, 0, 0, 0, 1, 0]   # IMU y velocity\n        ])\n        \n    def update(self, encoder_pos, imu_vel, dt):\n        # Predict using constant velocity model\n        self.kf.state_transition = np.array([\n            [1, 0, 0, dt, 0, 0],\n            [0, 1, 0, 0, dt, 0],\n            [0, 0, 1, 0, 0, dt],\n            [0, 0, 0, 1, 0, 0],\n            [0, 0, 0, 0, 1, 0],\n            [0, 0, 0, 0, 0, 1]\n        ])\n        \n        self.kf.predict()\n        \n        # Combine measurements\n        measurement = np.array([\n            encoder_pos[0], encoder_pos[1],\n            imu_vel[0], imu_vel[1]\n        ])\n        \n        return self.kf.update(measurement)\n\n# Usage example\nestimator = RobotPositionEstimator()\n\nfor i in range(10):\n    # Simulated sensor data\n    encoder_pos = [i * 0.1, i * 0.05]  # Position from wheel encoders\n    imu_vel = [0.1, 0.05]  # Velocity from IMU\n    \n    estimated_state = estimator.update(encoder_pos, imu_vel, dt=0.1)\n    print(f"Step {i}: Position ({estimated_state[0]:.3f}, {estimated_state[1]:.3f})")\n'})}),"\n",(0,r.jsx)(n.h3,{id:"complementary-filtering",children:"Complementary Filtering"}),"\n",(0,r.jsx)(n.p,{children:"Complementary filters combine high-frequency and low-frequency information from different sensors. They are computationally efficient and work well for many applications."}),"\n",(0,r.jsx)(n.h3,{id:"particle-filtering",children:"Particle Filtering"}),"\n",(0,r.jsx)(n.p,{children:"Particle filters use Monte Carlo methods to estimate probability distributions. They are particularly useful for:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Localization in known environments"}),"\n",(0,r.jsx)(n.li,{children:"Tracking multiple hypotheses"}),"\n",(0,r.jsx)(n.li,{children:"Non-linear system models"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"sensor-placement-strategies",children:"Sensor Placement Strategies"}),"\n",(0,r.jsx)(n.h3,{id:"humanoid-robot-sensor-layout",children:"Humanoid Robot Sensor Layout"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Head Sensors"}),": Vision cameras, microphones, and distance sensors are typically placed in the head to provide a human-like perspective and good vantage point for navigation."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Body Sensors"}),": IMUs are placed in the torso to measure overall body orientation and motion. Force sensors may be integrated into the torso for collision detection."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Joint Sensors"}),": Encoders and force sensors are placed at each joint to provide precise position and force information for control."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Hand Sensors"}),": Tactile sensors, pressure sensors, and force/torque sensors are integrated into hands for manipulation and grasping."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Foot Sensors"}),": Force sensors and pressure sensors in feet provide information about contact forces and balance, essential for stable walking."]}),"\n",(0,r.jsx)(n.h3,{id:"redundancy-and-robustness",children:"Redundancy and Robustness"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Sensor Redundancy"}),": Critical measurements often use multiple sensors to provide backup in case of failure and improve accuracy through fusion."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Cross-Validation"}),": Different sensors can validate each other's measurements, helping detect sensor failures or anomalies."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Graceful Degradation"}),": Systems should continue operating with reduced capability when some sensors fail, rather than complete failure."]}),"\n",(0,r.jsx)(n.h2,{id:"code-examples",children:"Code Examples"}),"\n",(0,r.jsx)(n.h3,{id:"sensor-data-processing-pipeline",children:"Sensor Data Processing Pipeline"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import numpy as np\nfrom collections import deque\n\nclass SensorProcessor:\n    """Generic sensor data processing pipeline"""\n    \n    def __init__(self, buffer_size=100):\n        self.buffer_size = buffer_size\n        self.data_buffer = deque(maxlen=buffer_size)\n        self.calibration_offset = 0.0\n        self.scale_factor = 1.0\n        \n    def calibrate(self, reference_values):\n        """Calibrate sensor using reference values"""\n        measurements = np.array(list(self.data_buffer))\n        self.calibration_offset = np.mean(reference_values) - np.mean(measurements)\n        self.scale_factor = np.std(reference_values) / np.std(measurements)\n        \n    def add_measurement(self, raw_value):\n        """Add new measurement to buffer"""\n        self.data_buffer.append(raw_value)\n        \n    def get_filtered_value(self, method=\'moving_average\'):\n        """Get filtered sensor reading"""\n        if len(self.data_buffer) == 0:\n            return 0.0\n            \n        measurements = np.array(list(self.data_buffer))\n        \n        if method == \'moving_average\':\n            return np.mean(measurements)\n        elif method == \'median\':\n            return np.median(measurements)\n        elif method == \'exponential\':\n            alpha = 0.1\n            filtered = measurements[0]\n            for value in measurements[1:]:\n                filtered = alpha * value + (1 - alpha) * filtered\n            return filtered\n        else:\n            return measurements[-1]\n            \n    def get_calibrated_value(self, raw_value):\n        """Apply calibration to raw measurement"""\n        return (raw_value + self.calibration_offset) * self.scale_factor\n\nclass MultiSensorFusion:\n    """Multi-sensor fusion system"""\n    \n    def __init__(self):\n        self.sensors = {}\n        self.fusion_weights = {}\n        \n    def add_sensor(self, name, sensor_processor, weight=1.0):\n        """Add sensor to fusion system"""\n        self.sensors[name] = sensor_processor\n        self.fusion_weights[name] = weight\n        \n    def get_fused_value(self):\n        """Get fused sensor reading"""\n        if not self.sensors:\n            return 0.0\n            \n        total_weight = sum(self.fusion_weights.values())\n        weighted_sum = 0.0\n        \n        for name, sensor in self.sensors.items():\n            filtered_value = sensor.get_filtered_value()\n            weight = self.fusion_weights[name]\n            weighted_sum += filtered_value * weight\n            \n        return weighted_sum / total_weight\n\n# Example usage for force sensing\nforce_sensor = SensorProcessor(buffer_size=50)\nforce_sensor.add_measurement(10.1)\nforce_sensor.add_measurement(10.3)\nforce_sensor.add_measurement(9.9)\nforce_sensor.add_measurement(10.2)\n\nfiltered_force = force_sensor.get_filtered_value(\'moving_average\')\nprint(f"Filtered force: {filtered_force:.2f} N")\n'})}),"\n",(0,r.jsx)(n.h3,{id:"vision-processing-for-object-detection",children:"Vision Processing for Object Detection"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import cv2\nimport numpy as np\n\nclass VisionProcessor:\n    """Computer vision processing for humanoid robots"""\n    \n    def __init__(self):\n        self.camera_matrix = None\n        self.distortion_coeffs = None\n        \n    def calibrate_camera(self, calibration_images):\n        """Calibrate camera using chessboard images"""\n        # Prepare object points\n        objp = np.zeros((6*9, 3), np.float32)\n        objp[:, :2] = np.mgrid[0:9, 0:6].T.reshape(-1, 2)\n        \n        objpoints = []\n        imgpoints = []\n        \n        for img in calibration_images:\n            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n            ret, corners = cv2.findChessboardCorners(gray, (9, 6), None)\n            \n            if ret:\n                objpoints.append(objp)\n                imgpoints.append(corners)\n        \n        # Calibrate camera\n        ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(\n            objpoints, imgpoints, gray.shape[::-1], None, None\n        )\n        \n        if ret:\n            self.camera_matrix = mtx\n            self.distortion_coeffs = dist\n            \n    def detect_objects(self, image):\n        """Detect objects in image using color and shape"""\n        # Convert to HSV for better color detection\n        hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n        \n        # Define color ranges for different objects\n        color_ranges = {\n            \'red\': [(0, 50, 50), (10, 255, 255)],\n            \'green\': [(50, 50, 50), (70, 255, 255)],\n            \'blue\': [(100, 50, 50), (130, 255, 255)]\n        }\n        \n        detected_objects = []\n        \n        for color_name, (lower, upper) in color_ranges.items():\n            # Create mask for color\n            mask = cv2.inRange(hsv, np.array(lower), np.array(upper))\n            \n            # Find contours\n            contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, \n                                          cv2.CHAIN_APPROX_SIMPLE)\n            \n            for contour in contours:\n                # Filter by area\n                area = cv2.contourArea(contour)\n                if area > 1000:  # Minimum area threshold\n                    # Get bounding box\n                    x, y, w, h = cv2.boundingRect(contour)\n                    \n                    # Calculate center\n                    center_x = x + w // 2\n                    center_y = y + h // 2\n                    \n                    detected_objects.append({\n                        \'type\': color_name,\n                        \'position\': (center_x, center_y),\n                        \'size\': (w, h),\n                        \'area\': area\n                    })\n                    \n        return detected_objects\n        \n    def estimate_distance(self, object_size, known_size, focal_length):\n        """Estimate distance to object using size"""\n        return (known_size * focal_length) / object_size\n\n# Example usage\nvision = VisionProcessor()\n\n# Simulate object detection\nimage = np.zeros((480, 640, 3), dtype=np.uint8)  # Black image\ncv2.rectangle(image, (200, 150), (300, 250), (0, 255, 0), -1)  # Green square\n\nobjects = vision.detect_objects(image)\nfor obj in objects:\n    print(f"Detected {obj[\'type\']} object at position {obj[\'position\']}")\n'})}),"\n",(0,r.jsx)(n.h2,{id:"practical-examples",children:"Practical Examples"}),"\n",(0,r.jsx)(n.h3,{id:"humanoid-robot-balance-control",children:"Humanoid Robot Balance Control"}),"\n",(0,r.jsx)(n.p,{children:"Balance control requires integrating multiple sensor inputs:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"IMU Data"}),": Provides body orientation and angular velocity"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Foot Force Sensors"}),": Measure center of pressure"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Joint Encoders"}),": Track joint positions for stability"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Vision"}),": Detect obstacles and terrain changes"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"The balance controller fuses these inputs to maintain stability:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'class BalanceController:\n    """Balance controller for humanoid robots"""\n    \n    def __init__(self):\n        self.kp_orientation = 100.0  # Orientation control gain\n        self.kp_cop = 50.0  # Center of pressure control gain\n        self.max_torque = 50.0  # Maximum joint torque\n        \n    def compute_balance_correction(self, imu_data, foot_forces, joint_positions):\n        """Compute joint torques for balance correction"""\n        # Extract orientation from IMU\n        roll, pitch, yaw = imu_data[\'orientation\']\n        \n        # Calculate center of pressure from foot forces\n        cop_x, cop_y = self.calculate_cop(foot_forces)\n        \n        # Compute desired corrections\n        roll_correction = -self.kp_orientation * roll\n        pitch_correction = -self.kp_orientation * pitch\n        \n        # Compute ankle torques for balance\n        left_ankle_torque = roll_correction + pitch_correction\n        right_ankle_torque = roll_correction - pitch_correction\n        \n        # Apply torque limits\n        left_ankle_torque = np.clip(left_ankle_torque, \n                                    -self.max_torque, self.max_torque)\n        right_ankle_torque = np.clip(right_ankle_torque, \n                                     -self.max_torque, self.max_torque)\n        \n        return {\n            \'left_ankle\': left_ankle_torque,\n            \'right_ankle\': right_ankle_torque\n        }\n        \n    def calculate_cop(self, foot_forces):\n        """Calculate center of pressure from foot force sensors"""\n        # Simplified calculation\n        total_force = sum(foot_forces.values())\n        if total_force == 0:\n            return 0.0, 0.0\n            \n        cop_x = sum(pos[0] * force for pos, force in foot_forces.items()) / total_force\n        cop_y = sum(pos[1] * force for pos, force in foot_forces.items()) / total_force\n        \n        return cop_x, cop_y\n'})}),"\n",(0,r.jsx)(n.h2,{id:"exercises",children:"Exercises"}),"\n",(0,r.jsx)(n.h3,{id:"exercise-1-sensor-selection-analysis",children:"Exercise 1: Sensor Selection Analysis"}),"\n",(0,r.jsx)(n.p,{children:"Select appropriate sensors for a humanoid robot designed for:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Household assistance tasks"}),"\n",(0,r.jsx)(n.li,{children:"Industrial manufacturing"}),"\n",(0,r.jsx)(n.li,{children:"Search and rescue operations"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Justify your choices based on performance requirements, environmental conditions, and cost constraints."}),"\n",(0,r.jsx)(n.h3,{id:"exercise-2-sensor-fusion-implementation",children:"Exercise 2: Sensor Fusion Implementation"}),"\n",(0,r.jsx)(n.p,{children:"Implement a sensor fusion system that combines:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Wheel encoder position data"}),"\n",(0,r.jsx)(n.li,{children:"IMU velocity data"}),"\n",(0,r.jsx)(n.li,{children:"Visual odometry from camera"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Compare the performance of different fusion algorithms (Kalman filter, complementary filter, particle filter)."}),"\n",(0,r.jsx)(n.h3,{id:"exercise-3-sensor-calibration",children:"Exercise 3: Sensor Calibration"}),"\n",(0,r.jsx)(n.p,{children:"Design a calibration procedure for a 6-DOF force/torque sensor. Include:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Static calibration with known loads"}),"\n",(0,r.jsx)(n.li,{children:"Dynamic calibration with time-varying forces"}),"\n",(0,r.jsx)(n.li,{children:"Temperature compensation"}),"\n",(0,r.jsx)(n.li,{children:"Validation procedures"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"exercise-4-fault-detection",children:"Exercise 4: Fault Detection"}),"\n",(0,r.jsx)(n.p,{children:"Implement a fault detection system for multiple sensors. The system should:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Detect sensor failures or anomalies"}),"\n",(0,r.jsx)(n.li,{children:"Identify which sensor has failed"}),"\n",(0,r.jsx)(n.li,{children:"Provide alternative estimates using remaining sensors"}),"\n",(0,r.jsx)(n.li,{children:"Alert operators about sensor issues"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,r.jsx)(n.p,{children:"Sensors are essential components that enable humanoid robots to perceive themselves and their environment. Proprioceptive sensors provide information about internal state, while exteroceptive sensors reveal the external world. Each sensor type has unique strengths and limitations, making sensor fusion crucial for robust perception."}),"\n",(0,r.jsx)(n.p,{children:"Modern humanoid robots integrate dozens of sensors, each carefully selected and positioned to provide comprehensive coverage of the robot's operational needs. The challenge lies not just in selecting appropriate sensors, but in processing their data efficiently, fusing information from multiple sources, and maintaining system reliability despite sensor failures or environmental disturbances."}),"\n",(0,r.jsx)(n.p,{children:"As sensor technology advances, we can expect more capable, compact, and intelligent sensing systems that will enable humanoid robots to operate more effectively in complex, unstructured environments. The future of humanoid robotics depends heavily on continued innovation in sensing technologies and perception algorithms."}),"\n",(0,r.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,r.jsxs)(n.p,{children:['[1] Thrun, S., Burgard, W., and Fox, D. "Probabilistic Robotics", MIT Press, 2022. ',(0,r.jsx)(n.a,{href:"https://doi.org/10.7551/mitpress/9780262201629.001.0001",children:"https://doi.org/10.7551/mitpress/9780262201629.001.0001"})]}),"\n",(0,r.jsxs)(n.p,{children:['[2] Szeliski, R. "Computer Vision: Algorithms and Applications", 2nd Edition, Springer, 2022. ',(0,r.jsx)(n.a,{href:"https://doi.org/10.1007/978-1-84882-935-0",children:"https://doi.org/10.1007/978-1-84882-935-0"})]}),"\n",(0,r.jsxs)(n.p,{children:['[3] Bar-Shalom, Y., Li, X.R., and Kirubarajan, T. "Estimation with Applications to Tracking and Navigation", Wiley, 2021. ',(0,r.jsx)(n.a,{href:"https://doi.org/10.1002/0471221279",children:"https://doi.org/10.1002/0471221279"})]}),"\n",(0,r.jsxs)(n.p,{children:['[4] ROS 2 Documentation, "Sensor Messages and Processing", 2025. ',(0,r.jsx)(n.a,{href:"https://docs.ros.org/en/rolling/",children:"https://docs.ros.org/en/rolling/"})]}),"\n",(0,r.jsxs)(n.p,{children:['[5] IEEE Robotics and Automation Society, "Sensor Standards for Robotics", 2024. ',(0,r.jsx)(n.a,{href:"https://www.ieee-ras.org/standards",children:"https://www.ieee-ras.org/standards"})]})]})}function p(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>t,x:()=>a});var i=s(6540);const r={},o=i.createContext(r);function t(e){const n=i.useContext(o);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:t(e.components),i.createElement(o.Provider,{value:n},e.children)}}}]);